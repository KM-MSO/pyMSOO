{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyMSOO.utils.Crossover import *\n",
    "from pyMSOO.utils.Mutation import *\n",
    "from pyMSOO.utils.Selection import *\n",
    "from pyMSOO.utils.DimensionAwareStrategy import *\n",
    "from pyMSOO.MFEA.benchmark.continous import *\n",
    "from pyMSOO.utils.MultiRun.RunMultiTime import * \n",
    "from pyMSOO.utils.MultiRun.RunMultiBenchmark import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks, IndClass = CEC17_benchmark.get_2tasks_benchmark(1)\n",
    "# tasks, IndClass = WCCI22_benchmark.get_complex_benchmark(1)\n",
    "tasks, IndClass = CEC17_benchmark.get_10tasks_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from numba import jit, njit\n",
    "\n",
    "from pyMSOO.MFEA.model import AbstractModel\n",
    "from pyMSOO.utils import Crossover, Mutation, Selection, DimensionAwareStrategy\n",
    "from pyMSOO.utils.EA import *\n",
    "from pyMSOO.utils.numba_utils import numba_randomchoice, numba_random_gauss, numba_random_cauchy, numba_random_uniform\n",
    "from pyMSOO.utils.Search import *\n",
    "\n",
    "class model(AbstractModel.model):\n",
    "    TOLERANCE = 1e-6\n",
    "    INF = 1e8\n",
    "    def compile(self, \n",
    "        IndClass: Type[Individual],\n",
    "        tasks: List[AbstractTask], \n",
    "        crossover: Crossover.SBX_Crossover, \n",
    "        mutation: Mutation.PolynomialMutation, \n",
    "        selection: Selection.ElitismSelection,\n",
    "        dimension_strategy: DimensionAwareStrategy.AbstractDaS = DimensionAwareStrategy.NoDaS(),\n",
    "        *args, **kwargs):\n",
    "        super().compile(IndClass, tasks, crossover, mutation,dimension_strategy, selection, *args, **kwargs)\n",
    "    \n",
    "    def fit(self, nb_generations, \n",
    "            nb_inds_each_task = 100, \n",
    "            nb_inds_max = 100,\n",
    "            nb_inds_min = 20,\n",
    "            evaluate_initial_skillFactor = True, \n",
    "            c = 0.06,\n",
    "            *args, \n",
    "            **kwargs) -> List[Individual]:\n",
    "        super().fit(*args, **kwargs)\n",
    "\n",
    "        # nb_inds_min\n",
    "        if nb_inds_min is not None:\n",
    "            assert nb_inds_each_task >= nb_inds_min\n",
    "        else: \n",
    "            nb_inds_min = nb_inds_each_task\n",
    "\n",
    "        self.rmp = np.full((len(self.tasks), len(self.tasks)), 0.3)\n",
    "        self.learningPhase = [LearningPhase(self.IndClass, self.tasks, t) for t in self.tasks]\n",
    "        \n",
    "        # initialize population\n",
    "        self.population = Population(\n",
    "            self.IndClass,\n",
    "            nb_inds_tasks = [nb_inds_each_task] * len(self.tasks), \n",
    "            dim = self.dim_uss,\n",
    "            list_tasks= self.tasks,\n",
    "            evaluate_initial_skillFactor = evaluate_initial_skillFactor\n",
    "        )\n",
    "\n",
    "        self.nb_inds_tasks = [nb_inds_each_task] * len(self.tasks)\n",
    "\n",
    "        MAXEVALS = nb_generations * nb_inds_each_task * len(self.tasks)\n",
    "        self.max_eval_k = [nb_generations * nb_inds_each_task] * len(self.tasks)\n",
    "        self.eval_k = [0] * len(self.tasks)\n",
    "        epoch = 1\n",
    "        \n",
    "        D0 = self.calculateD(population = np.array([[ind.genes for ind in sub.ls_inds] for sub in self.population]), \n",
    "                            population_fitness = np.array([sub.getFitness() for sub in self.population]),\n",
    "                            best = np.array([sub.__getBestIndividual__.genes for sub in self.population]),)\n",
    "\n",
    "        while sum(self.eval_k) < MAXEVALS:\n",
    "            self.delta = [[[] for _ in range(len(self.tasks))] for _ in range(len(self.tasks))]\n",
    "\n",
    "            self.s_rmp = [[[] for _ in range(len(self.tasks))] for _ in range(len(self.tasks))]\n",
    "\n",
    "            self.population.update_rank()\n",
    "            # print(self.eval_k)\n",
    "            if sum(self.eval_k) >= epoch * nb_inds_each_task * len(self.tasks):\n",
    "                # save history\n",
    "                self.history_cost.append([ind.fcost for ind in self.population.get_solves()])\n",
    "                self.render_process(epoch/nb_generations, ['Pop_size', 'Cost'], [[sum(self.nb_inds_tasks)], self.history_cost[-1]], use_sys= True)\n",
    "                epoch += 1\n",
    "\n",
    "            # offsprings = self.reproduction(sum(self.nb_inds_tasks), self.population)\n",
    "\n",
    "            # self.population = self.population + offsprings\n",
    "            # start = time.time()\n",
    "            matingPool = Population(\n",
    "                self.IndClass,\n",
    "                nb_inds_tasks = [0] * len(self.tasks), \n",
    "                dim = self.dim_uss,\n",
    "                list_tasks= self.tasks,\n",
    "                evaluate_initial_skillFactor = False\n",
    "            )\n",
    "\n",
    "            for idx in range(len(self.tasks)):\n",
    "                \n",
    "                idx_inds = np.argsort([ind.fcost for ind in self.population[idx].ls_inds])\n",
    "                \n",
    "                for i in idx_inds[:int(len(self.population[idx])/2)]:\n",
    "                    matingPool.__addIndividual__(self.population[idx].ls_inds[i])\n",
    "\n",
    "            offsprings = self.reproduction(len(self.population), matingPool)\n",
    "        \n",
    "            # # merge and update rank\n",
    "            self.population = matingPool + offsprings\n",
    "            self.population.update_rank()\n",
    "            # end = time.time()\n",
    "            # print(\"E: \", end - start)\n",
    "            # selection\n",
    "            self.nb_inds_tasks = [int(\n",
    "                int(max((nb_inds_min - nb_inds_max) * (sum(self.eval_k)/MAXEVALS) + nb_inds_max, nb_inds_min))\n",
    "            )] * len(self.tasks)\n",
    "            self.selection(self.population, self.nb_inds_tasks)\n",
    "\n",
    "            # update operators\n",
    "            self.crossover.update(population = self.population)\n",
    "            self.mutation.update(population = self.population)\n",
    "            self.dimension_strategy.update(population = self.population)\n",
    "            # start = time.time()\n",
    "            self.updateRMP(c)\n",
    "            # end = time.time()\n",
    "            # print(\"G: \", end - start)\n",
    "            # start = time.time()\n",
    "            self.phaseTwo(D0)\n",
    "            # end = time.time()\n",
    "            # print(\"G: \", end - start)\n",
    "        # self.phaseTwo(D0)\n",
    "        print('\\nEND!')\n",
    "\n",
    "        #solve \n",
    "        self.render_process(epoch/nb_generations, ['Pop_size', 'Cost'], [[sum(self.nb_inds_tasks)], self.history_cost[-1]], use_sys= True)\n",
    "        self.population.update_rank()\n",
    "        self.last_pop = self.population\n",
    "        return self.last_pop.get_solves()\n",
    "    \n",
    "    def reproduction(self, size: int, mating_pool: Population,) -> Population:\n",
    "        sub_size = int(size/len(self.tasks))\n",
    "       \n",
    "        offsprings = Population(self.IndClass,\n",
    "                                nb_inds_tasks = [0] * len(self.tasks), \n",
    "                                dim = self.dim_uss,\n",
    "                                list_tasks= self.tasks)\n",
    "        counter = np.zeros((len(self.tasks)))  \n",
    "\n",
    "        stopping = False\n",
    "        while not stopping:\n",
    "            pa, pb = mating_pool.__getRandomInds__(2)\n",
    "            ta = pa.skill_factor\n",
    "            tb = pb.skill_factor\n",
    "\n",
    "            if counter[ta] >= sub_size and counter[tb] >= sub_size:\n",
    "                continue\n",
    "\n",
    "            rmpValue = numba_random_gauss(mean = max(self.rmp[ta][tb], self.rmp[tb][ta]), sigma = 0.1)\n",
    "\n",
    "            if ta == tb:\n",
    "                # self.eval_k[ta] += 2\n",
    "\n",
    "                oa, ob = self.crossover(pa, pb)\n",
    "\n",
    "                oa.skill_factor = ta\n",
    "                ob.skill_factor = ta\n",
    "\n",
    "                if self.eval_k[ta] >= self.max_eval_k[ta]:\n",
    "                    oa.fcost = model.INF\n",
    "                else:\n",
    "                    self.eval_k[ta] += 1\n",
    "\n",
    "                offsprings.__addIndividual__(oa)\n",
    "\n",
    "                if self.eval_k[tb] >= self.max_eval_k[tb]:\n",
    "                    ob.fcost = model.INF\n",
    "                else:\n",
    "                    self.eval_k[tb] += 1\n",
    "\n",
    "                offsprings.__addIndividual__(ob)\n",
    "\n",
    "                counter[ta] += 2\n",
    "\n",
    "            elif random.random() <= rmpValue:\n",
    "                off = self.crossover(pa, pb)\n",
    "\n",
    "                for o in off:\n",
    "                    if counter[ta] < sub_size and random.random() < self.rmp[ta][tb]/(self.rmp[ta][tb] + self.rmp[tb][ta]):\n",
    "                        o.skill_factor = ta\n",
    "                        o = self.dimension_strategy(o, tb, pa)\n",
    "                        if self.eval_k[ta] >= self.max_eval_k[ta]:\n",
    "                            o.fcost = model.INF\n",
    "                        else:\n",
    "                            self.eval_k[ta] += 1\n",
    "                            o.fcost = self.tasks[ta](o)\n",
    "\n",
    "                        offsprings.__addIndividual__(o)\n",
    "                        \n",
    "                        counter[ta] += 1\n",
    "                        # self.eval_k[ta] += 1\n",
    "                        \n",
    "                        if pa.fcost > o.fcost:\n",
    "                            self.delta[ta][tb].append(pa.fcost - o.fcost)\n",
    "                            self.s_rmp[ta][tb].append(rmpValue)\n",
    "                    \n",
    "                    elif counter[tb] < sub_size:\n",
    "                        o.skill_factor = tb\n",
    "                        o = self.dimension_strategy(o, ta, pb)\n",
    "        \n",
    "                        if self.eval_k[tb] >= self.max_eval_k[tb]:\n",
    "                            o.fcost = model.INF\n",
    "                        else:\n",
    "                            self.eval_k[tb] += 1\n",
    "                            o.fcost = self.tasks[tb](o)\n",
    "\n",
    "                        offsprings.__addIndividual__(o)\n",
    "                        \n",
    "                        counter[tb] += 1\n",
    "                        # self.eval_k[tb] += 1\n",
    "\n",
    "                        if pb.fcost > o.fcost:\n",
    "                            self.delta[tb][ta].append(pb.fcost - o.fcost)\n",
    "                            self.s_rmp[tb][ta].append(rmpValue)\n",
    "\n",
    "            else:\n",
    "                if counter[ta] < sub_size:\n",
    "                    paa: Individual = self.population[ta].__getRandomItems__()\n",
    "\n",
    "                    # while np.array_equal(paa.genes, pa.genes):\n",
    "                    #     paa: Individual = self.population[ta].__getRandomItems__()\n",
    "                    \n",
    "                    oa, _ = self.crossover(pa, paa)\n",
    "                    oa.skill_factor = ta\n",
    "                    \n",
    "                    if self.eval_k[ta] >= self.max_eval_k[ta]:\n",
    "                        oa.fcost = model.INF\n",
    "                    else:\n",
    "                        self.eval_k[ta] += 1\n",
    "                        oa.fcost = self.tasks[ta](oa)\n",
    "\n",
    "                    offsprings.__addIndividual__(oa)\n",
    "\n",
    "                    counter[ta] += 1\n",
    "                    # self.eval_k[ta] += 1\n",
    "\n",
    "                if counter[tb] < sub_size:\n",
    "                    pbb: Individual = self.population[tb].__getRandomItems__()\n",
    "\n",
    "                    # while np.array_equal(pbb.genes, pb.genes):\n",
    "                    #     pbb: Individual = self.population[tb].__getRandomItems__()\n",
    "                    \n",
    "                    ob, _ = self.crossover(pb, pbb)\n",
    "                    ob.skill_factor = tb\n",
    "\n",
    "                    if self.eval_k[tb] >= self.max_eval_k[tb]:\n",
    "                        ob.fcost = model.INF\n",
    "                    else:\n",
    "                        self.eval_k[tb] += 1\n",
    "                        ob.fcost = self.tasks[tb](ob)\n",
    "\n",
    "                    offsprings.__addIndividual__(ob)\n",
    "                    \n",
    "                    counter[tb] += 1\n",
    "                    # self.eval_k[tb] += 1\n",
    "                    \n",
    "            stopping = sum(counter >= sub_size) == len(self.tasks)\n",
    "\n",
    "        return offsprings\n",
    "\n",
    "    def phaseTwo(self, D0):\n",
    "        fcosts = [sub.getFitness() for sub in self.population]\n",
    "        # start = time.time()\n",
    "        D = self.calculateD(population = np.array([[ind.genes for ind in sub.ls_inds]for sub in self.population]), \n",
    "                            population_fitness = np.array(fcosts),\n",
    "                            best = np.array([sub.__getBestIndividual__.genes for sub in self.population]),\n",
    "                            )\n",
    "        # end = time.time()\n",
    "        # print(\"A: \", end - start)\n",
    "        maxFit = np.max(fcosts, axis=1)\n",
    "        minFit = np.min(fcosts, axis=1)\n",
    "        maxDelta = maxFit - minFit + 1e-99\n",
    "\n",
    "        assert len(D) == len(maxDelta), \"Wrong shape. Got {} and {}\".format(D.shape, maxDelta.shape)\n",
    "        assert len(D) == len(self.tasks), \"Got wrong shape\"\n",
    "\n",
    "        sigma = np.where(D > D0, 0, 1 - D/D0)\n",
    "        nextPop = Population(IndClass = self.IndClass,\n",
    "                            dim = self.dim_uss,\n",
    "                            nb_inds_tasks=[0] * len(self.tasks),\n",
    "                            list_tasks=self.tasks)\n",
    "        # start = time.time()\n",
    "        for i in range(len(self.tasks)):\n",
    "            self.eval_k[i] += self.learningPhase[i].evolve(self.population[i], nextPop, sigma[i], maxDelta[i])\n",
    "        # end = time.time()\n",
    "        # print(\"B: \", end - start)\n",
    "        self.population = nextPop\n",
    "\n",
    "    def calculateD(self, population: np.array, population_fitness: np.array, best: np.array) -> np.array:\n",
    "        '''\n",
    "        Arguments include:\\n\n",
    "        + `population`: genes of the current population\n",
    "        + `population_fitness`: fitness of the current population\n",
    "        + `best`: the best gene of each subpop\n",
    "        + `nb_tasks`: number of tasks\n",
    "        '''\n",
    "        \n",
    "        D = np.empty((len(self.tasks)))\n",
    "        for i in range(len(self.tasks)):\n",
    "            gene_max = [np.max(population[i], axis = 1).tolist()] * self.dim_uss\n",
    "            gene_min = [np.min(population[i], axis = 1).tolist()] * self.dim_uss\n",
    "\n",
    "            D[i] = self.__class__._calculateD(np.array(gene_max).T, np.array(gene_min).T, population[i], population_fitness[i], best[i], model.TOLERANCE)\n",
    "        return D\n",
    "    \n",
    "    @jit(nopython = True, parallel = True, cache=True)\n",
    "    def _calculateD(gene_max: np.array, gene_min: np.array, subPop: np.array, subPop_fitness: np.array, best: np.array, TOLERANCE: float) -> float:\n",
    "            # gene_max = gene_max.flatten()\n",
    "            # gene_max = np.broadcast_to(gene_max, (subPop.shape[-1], subPop.shape[0])).T\n",
    "            # gene_min = gene_min.flatten()\n",
    "            # gene_min = np.broadcast_to(gene_min, (subPop.shape[-1], subPop.shape[0])).T\n",
    "            \n",
    "            w = np.where(subPop_fitness > TOLERANCE, 1/(subPop_fitness), 1/TOLERANCE)\n",
    "            # w = [1/ind if ind > TOLERANCE else 1/TOLERANCE for ind in population[i]]\n",
    "            # print(subPop.shape)\n",
    "            sum_w = sum(w)\n",
    "            d = (subPop - gene_min)/(gene_max - gene_min)\n",
    "            best = (best - gene_min)/(gene_max - gene_min)\n",
    "            d = np.sum((d - best) ** 2, axis=1)\n",
    "            d = np.sqrt(d)\n",
    "            assert d.shape == w.shape\n",
    "            # d = np.sqrt(np.sum(d, axis=0))\n",
    "            # d = np.sum([np.sqrt(np.sum((d[i] - best) * (d[i] - best))) for i in range(len(subPop))])\n",
    "\n",
    "            return np.sum(w * d/sum_w)\n",
    "    \n",
    "    def updateRMP(self, c: int):\n",
    "        for i in range(len(self.tasks)):\n",
    "            for j in range(len(self.tasks)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if len(self.delta[i][j]) > 0:\n",
    "                    self.rmp[i][j] += self.__class__._updateRMP(self.delta[i][j], self.s_rmp[i][j], c)\n",
    "                else:\n",
    "                    self.rmp[i][j] = (1 - c) * self.rmp[i][j]\n",
    "                \n",
    "                self.rmp[i][j] = max(0.1, min(1, self.rmp[i][j]))\n",
    "\n",
    "    @jit(nopython = True, parallel = True, cache= True)\n",
    "    def _updateRMP(delta: List, s_rmp: List, c: float) -> float:\n",
    "        delta = np.array(delta)\n",
    "        s_rmp = np.array(s_rmp)\n",
    "        sum_delta = sum(delta)\n",
    "        tmp = (delta/sum_delta) * s_rmp\n",
    "        meanS = sum(tmp * s_rmp)\n",
    "        \n",
    "        return c * meanS/sum(tmp)\n",
    "    \n",
    "class LearningPhase():\n",
    "    M = 2\n",
    "    H = 10\n",
    "    def __init__(self, IndClass, list_tasks, task) -> None:\n",
    "        self.IndClass = IndClass\n",
    "        self.list_tasks = list_tasks\n",
    "        self.task = task\n",
    "        self.sum_improv = [0.0] * LearningPhase.M\n",
    "        self.consume_fes = [1.0] * LearningPhase.M\n",
    "        self.mem_cr = [0.5] * LearningPhase.H\n",
    "        self.mem_f = [0.5] * LearningPhase.H\n",
    "        self.s_cr = []\n",
    "        self.s_f = []\n",
    "        self.diff_f = []\n",
    "        self.mem_pos = 0\n",
    "        self.gen = 0\n",
    "        self.best_opcode = 1\n",
    "        self.searcher = [self.pbest1, PolynomialMutation(nm = 5).getInforTasks(self.IndClass, self.list_tasks)]\n",
    "\n",
    "    def evolve(self, subPop: SubPopulation, nextPop: Population, sigma: float, max_delta: float) -> SubPopulation:\n",
    "        self.gen += 1\n",
    "\n",
    "        if self.gen > 1:\n",
    "            # start = time.time()\n",
    "            self.best_opcode = self.__class__.updateOperator(sum_improve = self.sum_improv, \n",
    "                                                             consume_fes = self.consume_fes, \n",
    "                                                             M = LearningPhase.M)\n",
    "\n",
    "            self.sum_improv = [0.0] * LearningPhase.M\n",
    "            self.consume_fes = [1.0] * LearningPhase.M\n",
    "\n",
    "            # end = time.time()\n",
    "            # print(\"C: \", end - start)\n",
    "\n",
    "        # self.updateMemory()\n",
    "        \n",
    "        pbest_size = max(5, int(0.15 * len(subPop)))\n",
    "        # pbest = subPop.__getRandomItems__(size = pbest_size)\n",
    "        idx_inds = np.argsort([ind.fcost for ind in subPop.ls_inds])\n",
    "        pbest =  [subPop.ls_inds[i] for i in idx_inds[:pbest_size]] \n",
    "        # start1 = time.time()\n",
    "        for ind in subPop:\n",
    "            # start1 = time.time()\n",
    "            # start = time.time()\n",
    "            r = random.randint(0, LearningPhase.M - 1)\n",
    "            cr = numba_random_gauss(self.mem_cr[r], 0.1)\n",
    "            f = numba_random_cauchy(self.mem_f[r], 0.1)\n",
    "            # end = time.time()\n",
    "            # print(\"A: \", end - start)\n",
    "            opcode = random.randint(0, LearningPhase.M)\n",
    "            if opcode == LearningPhase.M:\n",
    "                opcode = self.best_opcode\n",
    "\n",
    "            self.consume_fes[opcode] += 1\n",
    "            \n",
    "            if opcode == 0:\n",
    "                # start = time.time()\n",
    "                child = self.searcher[opcode](ind, subPop, pbest, cr, f)\n",
    "                # end = time.time()\n",
    "                # print(\"C: \", end - start)\n",
    "            elif opcode == 1:\n",
    "                # start = time.time()\n",
    "                child = self.searcher[opcode](ind, return_newInd=True)\n",
    "                # end = time.time()\n",
    "                # print(\"D: \", end - start)\n",
    "\n",
    "            # start = time.time()\n",
    "            child.skill_factor = ind.skill_factor\n",
    "            child.fcost = self.task(child)\n",
    "            \n",
    "            diff = ind.fcost - child.fcost\n",
    "            if diff > 0:\n",
    "                survival = child\n",
    "\n",
    "                self.sum_improv[opcode] += diff\n",
    "\n",
    "                if opcode == 0:\n",
    "                    self.diff_f.append(diff)\n",
    "                    # self.s_cr.append(cr)\n",
    "                    # self.s_f.append(f)\n",
    "                \n",
    "            elif diff == 0 or random.random() <= sigma * np.exp(diff/max_delta):\n",
    "                survival = child\n",
    "            else:\n",
    "                survival = ind\n",
    "            \n",
    "            nextPop.__addIndividual__(survival)\n",
    "            # end = time.time()\n",
    "            # print(\"M: \", end - start)\n",
    "        # end = time.time()\n",
    "        # print(\"F: \", end - start1)\n",
    "        return len(subPop)\n",
    "    \n",
    "    def pbest1(self, ind: Individual, subPop: SubPopulation, best: List[Individual], cr: float, f: float) -> Individual:\n",
    "        pbest = best[random.randint(0, len(best) - 1)]\n",
    "        \n",
    "        ind_ran1, ind_ran2 = subPop.__getRandomItems__(size = 2, replace= False)\n",
    "        \n",
    "        u = (numba_random_uniform(len(ind.genes)) < cr)\n",
    "        if np.sum(u) == 0:\n",
    "            u = np.zeros(shape= (subPop.dim,))\n",
    "            u[numba_randomchoice(subPop.dim)] = 1\n",
    "\n",
    "        # new_genes = np.where(u, \n",
    "        #     pbest.genes + f * (ind_ran1.genes - ind_ran2.genes),\n",
    "        #     ind.genes\n",
    "        # )\n",
    "        # # new_genes = np.clip(new_genes, ind.genes/2, (ind.genes + 1)/2)\n",
    "        # new_genes = np.where(new_genes < 0, ind.genes/2, np.where(new_genes > 1, (ind.genes + 1)/2, new_genes))\n",
    "\n",
    "        new_genes = self.__class__.produce_inds(ind.genes, pbest.genes, ind_ran1.genes, ind_ran2.genes, f, u)\n",
    "        new_ind = self.IndClass(new_genes)\n",
    "\n",
    "        return new_ind\n",
    "\n",
    "    @jit(nopython=True, parallel = True)\n",
    "    def produce_inds(ind_genes: np.array, best_genes: np.array, ind1_genes: np.array, ind2_genes: np.array, F: float, u: np.array) -> np.array:\n",
    "        new_genes = np.where(u,\n",
    "            best_genes + F * (ind1_genes - ind2_genes),\n",
    "            ind_genes\n",
    "        )\n",
    "        new_genes = np.where(new_genes > 1, (ind_genes + 1)/2, new_genes) \n",
    "        new_genes = np.where(new_genes < 0, (ind_genes + 0)/2, new_genes)\n",
    "\n",
    "        return new_genes\n",
    "\n",
    "    # def updateMemory(self):\n",
    "    #     if len(self.s_cr) > 0:\n",
    "    #         # self.diff_f = np.array(self.diff_f)\n",
    "    #         # self.s_cr = np.array(self.s_cr)\n",
    "    #         # self.s_f = np.array(self.s_f)\n",
    "\n",
    "    #         self.mem_cr[self.mem_pos] = self.__class__.updateMemoryCR(self.diff_f, self.s_cr)\n",
    "    #         self.mem_f[self.mem_pos] = self.__class__.updateMemoryF(self.diff_f, self.s_f)\n",
    "            \n",
    "    #         self.mem_pos = (self.mem_pos + 1) % LearningPhase.H\n",
    "\n",
    "    #         self.s_cr = []\n",
    "    #         self.s_f = []\n",
    "    #         self.diff_f = []\n",
    "\n",
    "    # @jit(nopython = True, parallel = True, cache=True)\n",
    "    # def updateMemoryCR(diff_f: List, s_cr: List) -> float:\n",
    "    #     diff_f = np.array(diff_f)\n",
    "    #     s_cr = np.array(s_cr)\n",
    "\n",
    "    #     sum_diff = sum(diff_f)\n",
    "    #     weight = diff_f/sum_diff\n",
    "    #     tmp_sum_cr = sum(weight * s_cr)\n",
    "    #     mem_cr = sum(weight * s_cr * s_cr)\n",
    "        \n",
    "    #     if tmp_sum_cr == 0 or mem_cr == -1:\n",
    "    #         return -1\n",
    "    #     else:\n",
    "    #         return mem_cr/tmp_sum_cr\n",
    "        \n",
    "    # @jit(nopython = True, parallel = True, cache = True)\n",
    "    # def updateMemoryF(diff_f: List, s_f: List) -> float:\n",
    "    #     diff_f = np.array(diff_f)\n",
    "    #     s_f = np.array(s_f)\n",
    "\n",
    "    #     sum_diff = sum(diff_f)\n",
    "    #     weight = diff_f/sum_diff\n",
    "    #     tmp_sum_f = sum(weight * s_f)\n",
    "    #     return sum(weight * (s_f ** 2)) / tmp_sum_f\n",
    "\n",
    "    @jit(nopython = True, parallel = True)\n",
    "    def updateOperator(sum_improve: List, consume_fes: List, M: int) -> int:\n",
    "        sum_improve = np.array(sum_improve)\n",
    "        consume_fes = np.array(consume_fes)\n",
    "        eta = sum_improve / consume_fes\n",
    "        best_rate = max(eta)\n",
    "        best_op = np.argmax(eta)\n",
    "        if best_rate > 0:\n",
    "            return best_op\n",
    "        else:\n",
    "            return random.randint(0, M - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'delta' of function 'model._updateRMP'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3020286771.py\", line 338:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True, cache= True)\n",
      "\u001b[1m    def _updateRMP(delta: List, s_rmp: List, c: float) -> float:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 's_rmp' of function 'model._updateRMP'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3020286771.py\", line 338:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True, cache= True)\n",
      "\u001b[1m    def _updateRMP(delta: List, s_rmp: List, c: float) -> float:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: None -- Time: 00m 7.72s    0 % [>                   ]  Pop_size: 9.90E+02  ,  Cost: 9.35E+04  2.33E+05  2.36E+05  3.58E+01  2.37E+09  2.13E+01  7.90E+01  1.65E+04  6.98E+01  5.89E+04  ,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'consume_fes' of function 'LearningPhase.updateOperator'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3020286771.py\", line 514:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True)\n",
      "\u001b[1m    def updateOperator(sum_improve: List, consume_fes: List, M: int) -> int:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'sum_improve' of function 'LearningPhase.updateOperator'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3020286771.py\", line 514:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True)\n",
      "\u001b[1m    def updateOperator(sum_improve: List, consume_fes: List, M: int) -> int:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: None -- Time: 02m 59.61s   99 % [===================>]  Pop_size: 2.00E+02  ,  Cost: 0.00E+00  1.94E-26  0.00E+00  0.00E+00  0.00E+00  4.31E-14  0.00E+00  6.36E-04  0.00E+00  2.79E+01  ,  \n",
      "END!\n",
      "Seed: None -- Time: 03m 1.29s  100 % [====================>]  Pop_size: 2.00E+02  ,  Cost: 0.00E+00  2.10E-26  0.00E+00  0.00E+00  0.00E+00  4.31E-14  0.00E+00  6.36E-04  0.00E+00  2.29E+01  ,  "
     ]
    }
   ],
   "source": [
    "baseModel = model()\n",
    "# from pyMSOO.MFEA.model import EME_BI\n",
    "# baseModel = EME_BI.model()\n",
    "baseModel.compile(\n",
    "    IndClass= IndClass,\n",
    "    tasks= tasks,\n",
    "    # crossover = KL_SBXCrossover(nc= 2, k= 100, conf_thres= 1),\n",
    "    crossover= SBX_Crossover(nc = 2),\n",
    "    mutation= PolynomialMutation(nm = 5),\n",
    "    selection= ElitismSelection(),\n",
    "    # dimension_strategy= DimensionAwareStrategy.DaS_strategy()\n",
    ")\n",
    "solve = baseModel.fit(\n",
    "    nb_generations = 1000, nb_inds_each_task= 100, \n",
    "    bound_pop= [0, 1], evaluate_initial_skillFactor= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(baseModel.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100020,\n",
       " 100020,\n",
       " 100021,\n",
       " 100020,\n",
       " 100023,\n",
       " 100020,\n",
       " 100020,\n",
       " 100020,\n",
       " 100020,\n",
       " 100022]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseModel.eval_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print([lp.best_opcode for lp in baseModel.learningPhase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: None -- Time: 00m 1.96s    0 % [>                   ]  Pop_size: 9.90E+02  ,  Cost: 1.02E+05  2.32E+05  2.40E+05  3.47E+01  2.87E+09  2.13E+01  7.91E+01  1.56E+04  5.74E+01  6.42E+04  ,  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m baseModel \u001b[39m=\u001b[39m model()\n\u001b[0;32m      2\u001b[0m baseModel\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      3\u001b[0m     IndClass\u001b[39m=\u001b[39m IndClass,\n\u001b[0;32m      4\u001b[0m     tasks\u001b[39m=\u001b[39m tasks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     dimension_strategy\u001b[39m=\u001b[39m DaS_strategy()\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m solve \u001b[39m=\u001b[39m baseModel\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     12\u001b[0m     nb_generations \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, nb_inds_each_task\u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, \n\u001b[0;32m     13\u001b[0m     bound_pop\u001b[39m=\u001b[39;49m [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m], evaluate_initial_skillFactor\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m )\n",
      "Cell \u001b[1;32mIn[34], line 117\u001b[0m, in \u001b[0;36mmodel.fit\u001b[1;34m(self, nb_generations, nb_inds_each_task, nb_inds_max, nb_inds_min, evaluate_initial_skillFactor, c, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimension_strategy\u001b[39m.\u001b[39mupdate(population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation)\n\u001b[0;32m    112\u001b[0m     \u001b[39m# start = time.time()\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[39m# self.updateRMP(c)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[39m# end = time.time()\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[39m# print(\"G: \", end - start)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# start = time.time()\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphaseTwo(D0)\n\u001b[0;32m    118\u001b[0m     \u001b[39m# end = time.time()\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[39m# print(\"G: \", end - start)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m# self.phaseTwo(D0)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEND!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 282\u001b[0m, in \u001b[0;36mmodel.phaseTwo\u001b[1;34m(self, D0)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39m# start = time.time()\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtasks)):\n\u001b[1;32m--> 282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_k[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearningPhase[i]\u001b[39m.\u001b[39;49mevolve(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulation[i], nextPop, sigma[i], maxDelta[i])\n\u001b[0;32m    283\u001b[0m \u001b[39m# end = time.time()\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m# print(\"B: \", end - start)\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation \u001b[39m=\u001b[39m nextPop\n",
      "Cell \u001b[1;32mIn[34], line 404\u001b[0m, in \u001b[0;36mLearningPhase.evolve\u001b[1;34m(self, subPop, nextPop, sigma, max_delta)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconsume_fes[opcode] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    402\u001b[0m \u001b[39mif\u001b[39;00m opcode \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    403\u001b[0m     \u001b[39m# start = time.time()\u001b[39;00m\n\u001b[1;32m--> 404\u001b[0m     child \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearcher[opcode](ind, subPop, pbest, cr, f)\n\u001b[0;32m    405\u001b[0m     \u001b[39m# end = time.time()\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[39m# print(\"C: \", end - start)\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39melif\u001b[39;00m opcode \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# start = time.time()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 457\u001b[0m, in \u001b[0;36mLearningPhase.pbest1\u001b[1;34m(self, ind, subPop, best, cr, f)\u001b[0m\n\u001b[0;32m    448\u001b[0m     u[numba_randomchoice(subPop\u001b[39m.\u001b[39mdim)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    450\u001b[0m \u001b[39m# new_genes = np.where(u, \u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m#     pbest.genes + f * (ind_ran1.genes - ind_ran2.genes),\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m#     ind.genes\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# # new_genes = np.clip(new_genes, ind.genes/2, (ind.genes + 1)/2)\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# new_genes = np.where(new_genes < 0, ind.genes/2, np.where(new_genes > 1, (ind.genes + 1)/2, new_genes))\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m new_genes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49mproduce_inds(ind\u001b[39m.\u001b[39;49mgenes, pbest\u001b[39m.\u001b[39;49mgenes, ind_ran1\u001b[39m.\u001b[39;49mgenes, ind_ran2\u001b[39m.\u001b[39;49mgenes, f, u)\n\u001b[0;32m    458\u001b[0m new_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mIndClass(new_genes)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m new_ind\n",
      "File \u001b[1;32mf:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\serialize.py:29\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[1;34m(address, bytedata, hashed)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Keep unpickled object via `numba_unpickle` alive.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _unpickled_memo \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[0;32m     30\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m        unpickled object\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     key \u001b[39m=\u001b[39m (address, hashed)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "baseModel = model()\n",
    "baseModel.compile(\n",
    "    IndClass= IndClass,\n",
    "    tasks= tasks,\n",
    "    # crossover = KL_SBXCrossover(nc= 2, k= 100, conf_thres= 1),\n",
    "    crossover= SBX_Crossover(nc = 2),\n",
    "    mutation= PolynomialMutation(nm = 5),\n",
    "    selection= ElitismSelection(),\n",
    "    dimension_strategy= DaS_strategy()\n",
    ")\n",
    "solve = baseModel.fit(\n",
    "    nb_generations = 1000, nb_inds_each_task= 100, \n",
    "    bound_pop= [0, 1], evaluate_initial_skillFactor= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"F:\\BTVN\\DSAI\\Optimization Lab\\Paper\\Efficient knowledge transfer\\history_cost_summaries_EME_BI_woDaS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>EME-BI</th>\n",
       "      <th>EME_BI_wo_DaS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.649250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>451</td>\n",
       "      <td>153.862380</td>\n",
       "      <td>140.877084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>453</td>\n",
       "      <td>10.890937</td>\n",
       "      <td>23.687499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>454</td>\n",
       "      <td>5022.383910</td>\n",
       "      <td>6610.532774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.183991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>456</td>\n",
       "      <td>112.875490</td>\n",
       "      <td>159.721696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>458</td>\n",
       "      <td>12.390112</td>\n",
       "      <td>21.295734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>459</td>\n",
       "      <td>4979.867545</td>\n",
       "      <td>6728.074941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>460</td>\n",
       "      <td>0.029993</td>\n",
       "      <td>16.883884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>461</td>\n",
       "      <td>128.012455</td>\n",
       "      <td>158.884861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>463</td>\n",
       "      <td>12.684814</td>\n",
       "      <td>21.087148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>464</td>\n",
       "      <td>5050.060552</td>\n",
       "      <td>5754.026860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>465</td>\n",
       "      <td>2.065367</td>\n",
       "      <td>15.545684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>466</td>\n",
       "      <td>136.240922</td>\n",
       "      <td>152.131004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>468</td>\n",
       "      <td>13.519725</td>\n",
       "      <td>20.299626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>469</td>\n",
       "      <td>4481.177414</td>\n",
       "      <td>6285.306503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>471</td>\n",
       "      <td>134.383059</td>\n",
       "      <td>162.881150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>473</td>\n",
       "      <td>9.980170</td>\n",
       "      <td>19.615759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>474</td>\n",
       "      <td>4404.093869</td>\n",
       "      <td>5303.187592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>475</td>\n",
       "      <td>0.685091</td>\n",
       "      <td>9.460654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>476</td>\n",
       "      <td>132.202483</td>\n",
       "      <td>166.484685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>478</td>\n",
       "      <td>11.444059</td>\n",
       "      <td>20.868381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>479</td>\n",
       "      <td>4755.577797</td>\n",
       "      <td>6082.033661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.058803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>481</td>\n",
       "      <td>152.043308</td>\n",
       "      <td>159.299178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>483</td>\n",
       "      <td>11.106148</td>\n",
       "      <td>24.428280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>484</td>\n",
       "      <td>4991.372559</td>\n",
       "      <td>6248.724829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.896160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>486</td>\n",
       "      <td>136.426152</td>\n",
       "      <td>169.301769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>488</td>\n",
       "      <td>10.483358</td>\n",
       "      <td>23.299838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>489</td>\n",
       "      <td>5397.686649</td>\n",
       "      <td>6405.145432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.494818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>491</td>\n",
       "      <td>147.769146</td>\n",
       "      <td>143.050375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>492</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>493</td>\n",
       "      <td>10.471335</td>\n",
       "      <td>20.319466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>5181.970735</td>\n",
       "      <td>6202.968885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>0.686529</td>\n",
       "      <td>15.653057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>113.118472</td>\n",
       "      <td>179.197397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>10.452321</td>\n",
       "      <td>21.174398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>4659.249712</td>\n",
       "      <td>5410.368813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       EME-BI  EME_BI_wo_DaS\n",
       "450         450     0.000000       0.649250\n",
       "451         451   153.862380     140.877084\n",
       "452         452     0.000000       0.000329\n",
       "453         453    10.890937      23.687499\n",
       "454         454  5022.383910    6610.532774\n",
       "455         455     0.000000       8.183991\n",
       "456         456   112.875490     159.721696\n",
       "457         457     0.000000       0.000000\n",
       "458         458    12.390112      21.295734\n",
       "459         459  4979.867545    6728.074941\n",
       "460         460     0.029993      16.883884\n",
       "461         461   128.012455     158.884861\n",
       "462         462     0.000000       0.000904\n",
       "463         463    12.684814      21.087148\n",
       "464         464  5050.060552    5754.026860\n",
       "465         465     2.065367      15.545684\n",
       "466         466   136.240922     152.131004\n",
       "467         467     0.000000       0.000000\n",
       "468         468    13.519725      20.299626\n",
       "469         469  4481.177414    6285.306503\n",
       "470         470     0.000000       0.651320\n",
       "471         471   134.383059     162.881150\n",
       "472         472     0.000000       0.000247\n",
       "473         473     9.980170      19.615759\n",
       "474         474  4404.093869    5303.187592\n",
       "475         475     0.685091       9.460654\n",
       "476         476   132.202483     166.484685\n",
       "477         477     0.000000       0.000000\n",
       "478         478    11.444059      20.868381\n",
       "479         479  4755.577797    6082.033661\n",
       "480         480     0.000000       4.058803\n",
       "481         481   152.043308     159.299178\n",
       "482         482     0.000000       0.000493\n",
       "483         483    11.106148      24.428280\n",
       "484         484  4991.372559    6248.724829\n",
       "485         485     0.000000      10.896160\n",
       "486         486   136.426152     169.301769\n",
       "487         487     0.000000       0.000000\n",
       "488         488    10.483358      23.299838\n",
       "489         489  5397.686649    6405.145432\n",
       "490         490     0.000000      11.494818\n",
       "491         491   147.769146     143.050375\n",
       "492         492     0.000247       0.000493\n",
       "493         493    10.471335      20.319466\n",
       "494         494  5181.970735    6202.968885\n",
       "495         495     0.686529      15.653057\n",
       "496         496   113.118472     179.197397\n",
       "497         497     0.000000       0.000000\n",
       "498         498    10.452321      21.174398\n",
       "499         499  4659.249712    5410.368813"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyMSOO.MFEA.model import EME_BI\n",
    "\n",
    "# ls_benchmark = []\n",
    "# ls_IndClass = []\n",
    "# name_benchmark = []\n",
    "# ls_tasks = [10]\n",
    "\n",
    "# for i in ls_tasks:\n",
    "#     # t, ic = WCCI22_benchmark.get_complex_benchmark(i)\n",
    "#     t, ic = WCCI22_benchmark.get_50tasks_benchmark(i)\n",
    "#     ls_benchmark.append(t)\n",
    "#     ls_IndClass.append(ic)\n",
    "#     name_benchmark.append(str(i))\n",
    "\n",
    "\n",
    "\n",
    "# smpModel = MultiBenchmark(\n",
    "#     ls_benchmark= ls_benchmark,\n",
    "#     name_benchmark= name_benchmark,\n",
    "#     ls_IndClass= ls_IndClass,\n",
    "#     model= EME_BI\n",
    "# )\n",
    "\n",
    "# smpModel.compile(\n",
    "#     crossover= SBX_Crossover(nc = 2),\n",
    "#     mutation= PolynomialMutation(nm = 5),\n",
    "#     selection= ElitismSelection(),\n",
    "#     # dimension_strategy = DaS_strategy(eta = 3)\n",
    "# )\n",
    "# smpModel.fit(\n",
    "#     nb_generations = 1000,nb_inds_each_task= 100, \n",
    "#     bound_pop= [0, 1], evaluate_initial_skillFactor= True\n",
    "# )\n",
    "# a = smpModel.run(\n",
    "#     nb_run= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  0.045998573303222656\n",
      "Seed: None -- Time: 00m 36.75s    0 % [>                   ]  Pop_size: 4.95E+03  ,  Cost: 2.12E+01  2.68E+04  2.76E+01  7.49E+01  1.50E+04  2.12E+01  2.59E+04  2.54E+01  7.42E+01  1.48E+04  2.13E+01  2.22E+04  2.14E+01  7.35E+01  1.48E+04  2.13E+01  2.44E+04  2.80E+01  7.58E+01  1.52E+04  2.13E+01  2.97E+04  2.76E+01  7.26E+01  1.38E+04  2.13E+01  2.78E+04  1.96E+01  7.12E+01  1.48E+04  2.13E+01  2.46E+04  2.59E+01  7.50E+01  1.49E+04  2.13E+01  2.65E+04  2.08E+01  7.25E+01  1.46E+04  2.13E+01  2.20E+04  2.70E+01  7.38E+01  1.47E+04  2.13E+01  2.03E+04  2.73E+01  7.48E+01  1.50E+04  ,  A:  0.07604598999023438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'consume_fes' of function 'LearningPhase.updateOperator'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3210406159.py\", line 480:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True)\n",
      "\u001b[1m    def updateOperator(sum_improve: List, consume_fes: List, M: int) -> int:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "f:\\Anaconda\\Anaconda\\envs\\MFEA\\lib\\site-packages\\numba\\core\\ir_utils.py:2147: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'sum_improve' of function 'LearningPhase.updateOperator'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_10276\\3210406159.py\", line 480:\u001b[0m\n",
      "\u001b[1m    @jit(nopython = True, parallel = True)\n",
      "\u001b[1m    def updateOperator(sum_improve: List, consume_fes: List, M: int) -> int:\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  0.06499814987182617\n",
      "A:  0.04599952697753906\n",
      "A:  0.042999982833862305\n",
      "A:  0.04296422004699707\n",
      "A:  0.04399561882019043\n",
      "A:  0.05096292495727539\n",
      "A:  0.04299807548522949\n",
      "A:  0.04499983787536621\n",
      "Seed: None -- Time: 00m 56.22s    1 % [>                   ]  Pop_size: 4.95E+03  ,  Cost: 2.12E+01  9.16E+03  7.83E+00  7.03E+01  1.43E+04  2.12E+01  7.87E+03  9.07E+00  7.06E+01  1.35E+04  2.13E+01  1.14E+04  1.03E+01  7.07E+01  1.42E+04  2.12E+01  9.56E+03  8.00E+00  7.24E+01  1.49E+04  2.13E+01  1.07E+04  1.15E+01  7.09E+01  1.38E+04  2.13E+01  9.61E+03  9.56E+00  6.86E+01  1.43E+04  2.12E+01  8.89E+03  9.93E+00  6.97E+01  1.33E+04  2.10E+01  9.22E+03  1.09E+01  7.05E+01  1.45E+04  2.13E+01  9.02E+03  8.50E+00  7.36E+01  1.47E+04  2.13E+01  9.75E+03  1.14E+01  7.11E+01  1.46E+04  ,  A:  0.047997236251831055\n",
      "A:  0.04296374320983887\n",
      "A:  0.04296684265136719\n",
      "A:  0.04196500778198242\n",
      "A:  0.04303455352783203\n",
      "A:  0.04296612739562988\n",
      "A:  0.04799985885620117\n",
      "A:  0.04799771308898926\n",
      "A:  0.04399919509887695\n",
      "A:  0.043999433517456055\n",
      "Seed: None -- Time: 01m 17.14s    2 % [>                   ]  Pop_size: 4.90E+03  ,  Cost: 2.12E+01  5.12E+03  5.77E+00  6.98E+01  1.28E+04  2.12E+01  5.32E+03  4.83E+00  6.76E+01  1.34E+04  2.13E+01  4.84E+03  4.26E+00  7.05E+01  1.36E+04  2.12E+01  5.21E+03  5.78E+00  6.77E+01  1.36E+04  2.12E+01  4.37E+03  5.48E+00  6.99E+01  1.32E+04  2.12E+01  4.60E+03  4.19E+00  5.92E+01  1.39E+04  2.12E+01  4.73E+03  5.72E+00  6.95E+01  1.29E+04  2.10E+01  5.34E+03  4.63E+00  6.93E+01  1.41E+04  2.13E+01  5.23E+03  3.76E+00  7.07E+01  1.44E+04  2.12E+01  5.01E+03  5.75E+00  6.83E+01  1.37E+04  ,  A:  0.04699969291687012\n",
      "A:  0.04296517372131348\n",
      "A:  0.04396700859069824\n",
      "A:  0.04403233528137207\n",
      "A:  0.04399991035461426\n",
      "A:  0.04299306869506836\n",
      "A:  0.04403328895568848\n",
      "A:  0.04399704933166504\n",
      "A:  0.051985740661621094\n",
      "A:  0.043997764587402344\n",
      "Seed: None -- Time: 01m 35.81s    3 % [>                   ]  Pop_size: 4.85E+03  ,  Cost: 2.12E+01  2.20E+03  3.23E+00  6.87E+01  1.28E+04  2.12E+01  2.51E+03  3.01E+00  6.55E+01  1.31E+04  2.13E+01  2.37E+03  3.24E+00  6.59E+01  1.33E+04  2.12E+01  2.69E+03  3.40E+00  6.74E+01  1.33E+04  2.11E+01  2.75E+03  3.53E+00  6.98E+01  1.30E+04  2.12E+01  2.79E+03  3.13E+00  5.33E+01  1.35E+04  2.11E+01  2.43E+03  3.27E+00  6.93E+01  1.29E+04  2.10E+01  2.68E+03  3.00E+00  6.42E+01  1.36E+04  2.13E+01  2.86E+03  2.77E+00  6.81E+01  1.41E+04  2.12E+01  2.88E+03  3.06E+00  6.77E+01  1.36E+04  ,  A:  0.04296588897705078\n",
      "A:  0.047003984451293945\n",
      "A:  0.042999982833862305\n",
      "A:  0.044999122619628906\n",
      "A:  0.04304170608520508\n",
      "A:  0.04296588897705078\n",
      "A:  0.0429990291595459\n",
      "A:  0.043996334075927734\n",
      "A:  0.042999982833862305\n",
      "A:  0.043964385986328125\n",
      "Seed: None -- Time: 01m 54.49s    4 % [>                   ]  Pop_size: 4.80E+03  ,  Cost: 2.12E+01  1.47E+03  2.10E+00  6.75E+01  1.27E+04  2.12E+01  1.93E+03  2.02E+00  6.45E+01  1.25E+04  2.12E+01  1.89E+03  2.19E+00  6.31E+01  1.31E+04  2.11E+01  1.78E+03  2.16E+00  6.19E+01  1.33E+04  2.11E+01  1.90E+03  2.12E+00  6.74E+01  1.26E+04  2.12E+01  1.67E+03  2.11E+00  5.17E+01  1.23E+04  2.11E+01  1.82E+03  2.32E+00  6.17E+01  1.29E+04  2.10E+01  1.79E+03  2.29E+00  6.11E+01  1.32E+04  2.13E+01  1.64E+03  2.02E+00  6.46E+01  1.34E+04  2.11E+01  1.89E+03  2.50E+00  6.47E+01  1.27E+04  ,  A:  0.043999671936035156\n",
      "A:  0.04196047782897949\n",
      "A:  0.04599905014038086\n",
      "A:  0.04900074005126953\n",
      "A:  0.04396772384643555\n",
      "A:  0.047997474670410156\n",
      "A:  0.042998313903808594\n",
      "A:  0.04196286201477051\n",
      "A:  0.04296445846557617\n",
      "A:  0.043998003005981445\n",
      "Seed: None -- Time: 02m 12.94s    5 % [=>                  ]  Pop_size: 4.75E+03  ,  Cost: 2.12E+01  9.93E+02  1.68E+00  6.43E+01  1.20E+04  2.12E+01  1.09E+03  1.49E+00  6.21E+01  1.22E+04  2.11E+01  1.18E+03  1.56E+00  6.27E+01  1.21E+04  2.05E+01  1.24E+03  1.68E+00  6.32E+01  1.25E+04  2.11E+01  1.16E+03  1.82E+00  6.63E+01  1.03E+04  2.12E+01  1.09E+03  1.79E+00  4.59E+01  1.20E+04  2.11E+01  1.31E+03  1.80E+00  5.42E+01  1.28E+04  2.10E+01  1.31E+03  1.65E+00  5.58E+01  1.29E+04  2.12E+01  1.21E+03  1.61E+00  6.39E+01  1.34E+04  2.11E+01  1.24E+03  1.81E+00  5.94E+01  1.16E+04  ,  A:  0.04503226280212402\n",
      "A:  0.04499626159667969\n",
      "A:  0.04399728775024414\n",
      "A:  0.04799795150756836\n",
      "A:  0.042966365814208984\n",
      "A:  0.05196666717529297\n",
      "A:  0.04999971389770508\n",
      "A:  0.04199838638305664\n",
      "A:  0.04396462440490723\n",
      "A:  0.042998552322387695\n",
      "Seed: None -- Time: 02m 32.12s    6 % [=>                  ]  Pop_size: 4.70E+03  ,  Cost: 2.11E+01  8.04E+02  1.42E+00  6.12E+01  1.20E+04  2.07E+01  1.05E+03  1.37E+00  6.12E+01  1.21E+04  2.07E+01  8.47E+02  1.38E+00  6.06E+01  1.20E+04  1.84E+01  9.26E+02  1.40E+00  5.82E+01  1.25E+04  2.11E+01  9.66E+02  1.58E+00  6.44E+01  9.76E+03  2.10E+01  8.50E+02  1.37E+00  3.67E+01  1.18E+04  2.10E+01  1.00E+03  1.49E+00  5.16E+01  1.24E+04  2.10E+01  9.45E+02  1.41E+00  5.56E+01  1.20E+04  2.12E+01  8.78E+02  1.41E+00  5.52E+01  1.32E+04  2.05E+01  8.77E+02  1.57E+00  5.54E+01  1.15E+04  ,  A:  0.04699993133544922\n",
      "A:  0.04296731948852539\n",
      "A:  0.043965816497802734\n",
      "A:  0.04700183868408203\n",
      "A:  0.05003237724304199\n",
      "A:  0.04199981689453125\n",
      "A:  0.043994903564453125\n",
      "A:  0.042999267578125\n",
      "A:  0.04296588897705078\n",
      "A:  0.042000532150268555\n",
      "Seed: None -- Time: 02m 50.77s    7 % [=>                  ]  Pop_size: 4.65E+03  ,  Cost: 2.08E+01  6.48E+02  1.23E+00  6.20E+01  1.28E+04  1.92E+01  7.86E+02  1.23E+00  5.86E+01  1.10E+04  1.94E+01  7.16E+02  1.16E+00  5.44E+01  1.18E+04  1.71E+01  7.26E+02  1.25E+00  5.18E+01  1.23E+04  2.05E+01  7.83E+02  1.24E+00  6.16E+01  9.71E+03  2.11E+01  8.19E+02  1.23E+00  3.38E+01  1.18E+04  2.08E+01  7.56E+02  1.27E+00  4.53E+01  1.22E+04  2.11E+01  7.09E+02  1.24E+00  5.20E+01  1.08E+04  2.12E+01  7.05E+02  1.28E+00  5.17E+01  1.26E+04  1.88E+01  6.98E+02  1.30E+00  5.15E+01  1.14E+04  ,  A:  0.05799674987792969\n",
      "A:  0.04499459266662598\n",
      "A:  0.04299759864807129\n",
      "A:  0.04399824142456055\n",
      "A:  0.04299664497375488\n",
      "A:  0.048003196716308594\n",
      "A:  0.04200005531311035\n",
      "A:  0.04296708106994629\n",
      "A:  0.04500174522399902\n",
      "A:  0.0410003662109375\n",
      "Seed: None -- Time: 03m 9.46s    8 % [=>                  ]  Pop_size: 4.60E+03  ,  Cost: 1.97E+01  6.03E+02  1.14E+00  5.70E+01  1.11E+04  1.74E+01  6.57E+02  1.13E+00  5.43E+01  9.94E+03  1.78E+01  6.24E+02  1.12E+00  5.35E+01  1.00E+04  1.43E+01  6.56E+02  1.15E+00  4.81E+01  1.23E+04  1.96E+01  6.80E+02  1.19E+00  5.41E+01  9.64E+03  2.07E+01  6.82E+02  1.16E+00  3.33E+01  1.14E+04  1.96E+01  6.94E+02  1.19E+00  4.17E+01  1.09E+04  2.10E+01  6.25E+02  1.15E+00  4.42E+01  9.97E+03  2.10E+01  6.16E+02  1.17E+00  5.06E+01  1.23E+04  1.65E+01  6.89E+02  1.24E+00  4.82E+01  1.11E+04  ,  A:  0.040998220443725586\n",
      "A:  0.0489962100982666\n",
      "A:  0.04196333885192871\n",
      "A:  0.04095935821533203\n",
      "A:  0.04296398162841797\n",
      "A:  0.0419917106628418\n",
      "A:  0.041007280349731445\n",
      "A:  0.04296135902404785\n",
      "A:  0.04203343391418457\n",
      "A:  0.06899619102478027\n",
      "Seed: None -- Time: 03m 28.13s    9 % [=>                  ]  Pop_size: 4.60E+03  ,  Cost: 1.78E+01  5.48E+02  1.09E+00  5.51E+01  1.10E+04  1.48E+01  6.33E+02  1.08E+00  4.84E+01  9.53E+03  1.56E+01  5.68E+02  1.08E+00  4.92E+01  9.45E+03  1.21E+01  5.38E+02  1.09E+00  4.35E+01  1.05E+04  1.69E+01  5.89E+02  1.13E+00  5.06E+01  9.62E+03  1.97E+01  5.73E+02  1.08E+00  3.26E+01  1.14E+04  1.70E+01  5.38E+02  1.11E+00  3.65E+01  1.05E+04  2.10E+01  6.13E+02  1.08E+00  4.27E+01  8.93E+03  1.99E+01  5.92E+02  1.12E+00  4.79E+01  1.23E+04  1.42E+01  5.98E+02  1.11E+00  4.59E+01  9.83E+03  ,  A:  0.04205060005187988\n",
      "A:  0.04300212860107422\n",
      "A:  0.045998334884643555\n",
      "A:  0.04200029373168945\n",
      "A:  0.04099845886230469\n",
      "A:  0.04500102996826172\n",
      "A:  0.04096364974975586\n",
      "A:  0.04196739196777344\n",
      "A:  0.04196524620056152\n",
      "A:  0.05500030517578125\n",
      "Seed: None -- Time: 03m 47.22s   10 % [==>                 ]  Pop_size: 4.55E+03  ,  Cost: 1.64E+01  5.59E+02  1.06E+00  5.12E+01  9.85E+03  1.32E+01  5.66E+02  1.03E+00  4.07E+01  9.03E+03  1.42E+01  5.17E+02  1.05E+00  4.49E+01  8.26E+03  1.14E+01  6.22E+02  1.03E+00  4.26E+01  9.29E+03  1.60E+01  5.33E+02  1.07E+00  4.47E+01  9.07E+03  1.83E+01  5.90E+02  1.05E+00  2.72E+01  1.11E+04  1.62E+01  5.68E+02  1.06E+00  2.95E+01  1.05E+04  2.09E+01  5.95E+02  1.04E+00  3.74E+01  8.00E+03  1.83E+01  5.71E+02  1.06E+00  4.42E+01  1.22E+04  1.21E+01  5.80E+02  1.08E+00  4.34E+01  8.69E+03  ,  A:  0.05199599266052246\n",
      "A:  0.04400014877319336\n",
      "A:  0.04901480674743652\n",
      "A:  0.04399919509887695\n",
      "A:  0.044995784759521484\n",
      "A:  0.04204273223876953\n",
      "A:  0.04900169372558594\n",
      "A:  0.04396414756774902\n",
      "A:  0.05199599266052246\n",
      "A:  0.04096555709838867\n",
      "Seed: None -- Time: 04m 6.76s   11 % [==>                 ]  Pop_size: 4.50E+03  ,  Cost: 1.39E+01  4.91E+02  1.04E+00  4.41E+01  9.04E+03  1.12E+01  5.81E+02  9.84E-01  3.66E+01  8.89E+03  1.29E+01  5.45E+02  1.02E+00  4.27E+01  8.24E+03  9.94E+00  5.79E+02  1.03E+00  3.97E+01  8.11E+03  1.29E+01  5.89E+02  1.04E+00  4.23E+01  7.79E+03  1.66E+01  4.94E+02  1.01E+00  2.61E+01  9.91E+03  1.45E+01  5.26E+02  1.04E+00  2.94E+01  9.67E+03  2.08E+01  5.73E+02  1.01E+00  3.50E+01  7.58E+03  1.53E+01  5.43E+02  1.03E+00  4.45E+01  1.00E+04  1.06E+01  5.40E+02  1.03E+00  3.91E+01  8.16E+03  ,  A:  0.04798102378845215\n",
      "A:  0.043996572494506836\n",
      "A:  0.056993961334228516\n",
      "A:  0.04099273681640625\n",
      "A:  0.0410006046295166\n",
      "A:  0.039962053298950195\n",
      "A:  0.045000553131103516\n",
      "A:  0.038965702056884766\n",
      "A:  0.038965702056884766\n",
      "A:  0.046002864837646484\n",
      "Seed: None -- Time: 04m 25.33s   12 % [==>                 ]  Pop_size: 4.45E+03  ,  Cost: 1.14E+01  5.58E+02  1.01E+00  4.09E+01  8.16E+03  1.03E+01  5.31E+02  8.87E-01  3.63E+01  8.81E+03  1.06E+01  5.04E+02  9.99E-01  3.90E+01  7.32E+03  8.67E+00  5.19E+02  9.94E-01  3.58E+01  7.38E+03  1.13E+01  5.47E+02  1.03E+00  3.91E+01  6.96E+03  1.46E+01  5.21E+02  9.63E-01  2.37E+01  8.65E+03  1.30E+01  5.19E+02  1.01E+00  2.88E+01  9.04E+03  2.08E+01  5.68E+02  9.41E-01  3.08E+01  7.14E+03  1.42E+01  5.07E+02  9.69E-01  4.03E+01  8.16E+03  8.92E+00  4.84E+02  9.91E-01  3.45E+01  7.74E+03  ,  A:  0.0409846305847168\n",
      "A:  0.03996634483337402\n",
      "A:  0.04499244689941406\n",
      "A:  0.04199934005737305\n",
      "A:  0.04900026321411133\n",
      "A:  0.04400014877319336\n",
      "A:  0.03896641731262207\n",
      "A:  0.03999590873718262\n",
      "A:  0.039998769760131836\n",
      "A:  0.03901028633117676\n",
      "Seed: None -- Time: 04m 43.32s   13 % [==>                 ]  Pop_size: 4.40E+03  ,  Cost: 9.79E+00  5.11E+02  9.80E-01  3.95E+01  7.93E+03  8.32E+00  5.70E+02  8.35E-01  2.88E+01  8.72E+03  8.86E+00  5.44E+02  9.54E-01  3.66E+01  6.95E+03  7.45E+00  5.46E+02  9.30E-01  3.33E+01  6.98E+03  1.04E+01  5.34E+02  9.04E-01  3.67E+01  6.30E+03  1.23E+01  5.39E+02  9.16E-01  2.22E+01  8.06E+03  1.08E+01  5.49E+02  9.36E-01  2.63E+01  8.50E+03  2.07E+01  5.53E+02  9.23E-01  2.89E+01  7.02E+03  1.23E+01  4.89E+02  9.77E-01  3.86E+01  7.22E+03  7.87E+00  5.24E+02  9.85E-01  3.24E+01  7.43E+03  ,  A:  0.04096579551696777\n",
      "A:  0.046004295349121094\n",
      "A:  0.04096722602844238\n",
      "A:  0.03899884223937988\n",
      "A:  0.04296422004699707\n",
      "A:  0.05000185966491699\n",
      "A:  0.05699419975280762\n",
      "A:  0.03899836540222168\n",
      "A:  0.03999662399291992\n",
      "A:  0.043001413345336914\n",
      "Seed: None -- Time: 05m 1.63s   14 % [==>                 ]  Pop_size: 4.40E+03  ,  Cost: 8.67E+00  5.61E+02  9.58E-01  3.63E+01  7.70E+03  7.51E+00  5.45E+02  8.13E-01  2.52E+01  8.68E+03  7.70E+00  5.44E+02  8.31E-01  3.42E+01  6.70E+03  6.15E+00  5.77E+02  9.66E-01  3.24E+01  6.69E+03  8.97E+00  4.97E+02  9.52E-01  3.47E+01  5.72E+03  9.39E+00  4.99E+02  9.88E-01  2.09E+01  7.65E+03  8.86E+00  5.37E+02  8.89E-01  2.09E+01  8.26E+03  2.05E+01  5.54E+02  7.75E-01  2.84E+01  6.80E+03  1.05E+01  5.14E+02  8.73E-01  3.50E+01  6.58E+03  6.62E+00  5.64E+02  9.33E-01  3.15E+01  7.40E+03  ,  A:  0.04099607467651367\n",
      "A:  0.03896474838256836\n",
      "A:  0.04499459266662598\n",
      "A:  0.03901243209838867\n",
      "A:  0.04099392890930176\n",
      "A:  0.04400634765625\n",
      "A:  0.04400014877319336\n",
      "A:  0.04096698760986328\n",
      "A:  0.04799795150756836\n",
      "A:  0.0409693717956543\n",
      "Seed: None -- Time: 05m 19.53s   15 % [===>                ]  Pop_size: 4.35E+03  ,  Cost: 7.72E+00  5.30E+02  8.13E-01  3.39E+01  7.59E+03  6.09E+00  5.09E+02  8.38E-01  2.53E+01  8.63E+03  6.19E+00  5.35E+02  9.30E-01  3.14E+01  6.06E+03  5.51E+00  5.29E+02  9.05E-01  3.06E+01  6.49E+03  7.52E+00  5.12E+02  9.03E-01  3.41E+01  5.24E+03  8.71E+00  5.36E+02  9.73E-01  1.93E+01  7.32E+03  8.20E+00  5.46E+02  8.55E-01  2.26E+01  8.06E+03  2.04E+01  5.26E+02  8.36E-01  2.58E+01  6.57E+03  8.51E+00  5.08E+02  8.53E-01  3.43E+01  5.78E+03  5.82E+00  5.28E+02  8.33E-01  3.15E+01  7.30E+03  ,  A:  0.03897213935852051\n",
      "A:  0.03999686241149902\n",
      "A:  0.04996299743652344\n",
      "A:  0.03999447822570801\n",
      "A:  0.04099583625793457\n",
      "A:  0.039006710052490234\n",
      "A:  0.03800225257873535\n",
      "A:  0.03896474838256836\n",
      "A:  0.03899955749511719\n",
      "A:  0.03796267509460449\n",
      "Seed: None -- Time: 05m 37.04s   16 % [===>                ]  Pop_size: 4.30E+03  ,  Cost: 6.67E+00  4.84E+02  8.45E-01  3.10E+01  7.53E+03  5.17E+00  5.19E+02  8.77E-01  2.41E+01  8.61E+03  5.61E+00  5.52E+02  8.72E-01  2.64E+01  5.55E+03  4.60E+00  5.19E+02  9.63E-01  2.84E+01  6.43E+03  6.65E+00  5.02E+02  9.24E-01  3.11E+01  4.91E+03  6.94E+00  5.30E+02  9.02E-01  1.69E+01  7.13E+03  7.33E+00  4.91E+02  8.09E-01  2.05E+01  7.89E+03  2.04E+01  5.42E+02  8.20E-01  2.41E+01  6.52E+03  7.29E+00  4.67E+02  8.81E-01  3.28E+01  5.65E+03  4.82E+00  4.73E+02  9.00E-01  3.15E+01  7.19E+03  ,  A:  0.03896355628967285\n",
      "A:  0.03896474838256836\n",
      "A:  0.039997100830078125\n",
      "A:  0.03896498680114746\n",
      "A:  0.03898453712463379\n",
      "A:  0.04096055030822754\n",
      "A:  0.03699803352355957\n",
      "A:  0.03699922561645508\n",
      "A:  0.03802061080932617\n",
      "A:  0.0379638671875\n",
      "Seed: None -- Time: 05m 54.56s   17 % [===>                ]  Pop_size: 4.25E+03  ,  Cost: 5.83E+00  4.93E+02  8.01E-01  2.80E+01  7.43E+03  4.67E+00  4.98E+02  7.82E-01  2.04E+01  8.59E+03  4.87E+00  5.55E+02  8.25E-01  2.98E+01  5.33E+03  4.17E+00  4.65E+02  9.42E-01  2.65E+01  6.32E+03  5.90E+00  4.97E+02  8.88E-01  3.00E+01  4.71E+03  6.53E+00  4.96E+02  9.30E-01  1.60E+01  6.80E+03  6.10E+00  5.32E+02  8.79E-01  1.97E+01  7.80E+03  2.04E+01  5.86E+02  7.10E-01  2.43E+01  6.46E+03  6.56E+00  5.31E+02  8.22E-01  3.16E+01  5.42E+03  4.36E+00  4.69E+02  8.38E-01  2.91E+01  7.11E+03  ,  A:  0.04096364974975586\n",
      "A:  0.03799772262573242\n",
      "A:  0.04099917411804199\n",
      "A:  0.038001298904418945\n",
      "A:  0.03999948501586914\n",
      "A:  0.03895878791809082\n",
      "A:  0.03799772262573242\n",
      "A:  0.039000511169433594\n",
      "A:  0.036982059478759766\n",
      "A:  0.03696584701538086\n",
      "Seed: None -- Time: 06m 11.61s   18 % [===>                ]  Pop_size: 4.20E+03  ,  Cost: 5.03E+00  4.94E+02  7.62E-01  2.74E+01  7.41E+03  4.10E+00  5.47E+02  7.81E-01  1.89E+01  8.58E+03  3.91E+00  5.29E+02  8.02E-01  2.74E+01  5.20E+03  3.26E+00  4.63E+02  8.73E-01  2.51E+01  6.29E+03  5.33E+00  5.01E+02  9.02E-01  2.96E+01  4.63E+03  5.93E+00  5.05E+02  8.28E-01  1.45E+01  6.70E+03  5.88E+00  5.19E+02  8.48E-01  1.80E+01  7.76E+03  2.03E+01  4.98E+02  6.66E-01  2.36E+01  6.43E+03  5.16E+00  5.21E+02  7.98E-01  2.67E+01  5.37E+03  4.10E+00  5.08E+02  9.05E-01  3.02E+01  7.08E+03  ,  A:  0.04000067710876465\n",
      "A:  0.038033485412597656\n",
      "A:  0.038964033126831055\n",
      "A:  0.03799796104431152\n",
      "A:  0.03899884223937988\n",
      "A:  0.03999733924865723\n",
      "A:  0.03900003433227539\n",
      "A:  0.035997629165649414\n",
      "A:  0.03999614715576172\n",
      "A:  0.03700065612792969\n",
      "Seed: None -- Time: 06m 28.65s   19 % [===>                ]  Pop_size: 4.20E+03  ,  Cost: 3.91E+00  5.31E+02  7.66E-01  2.61E+01  7.39E+03  3.77E+00  5.34E+02  7.38E-01  1.67E+01  8.58E+03  3.67E+00  5.31E+02  7.63E-01  2.58E+01  5.12E+03  3.01E+00  5.02E+02  8.72E-01  2.23E+01  6.25E+03  4.86E+00  4.65E+02  8.45E-01  2.86E+01  4.61E+03  4.53E+00  5.05E+02  8.48E-01  1.34E+01  6.65E+03  5.10E+00  4.88E+02  7.85E-01  1.62E+01  7.72E+03  2.03E+01  5.23E+02  7.20E-01  2.11E+01  6.41E+03  5.12E+00  5.18E+02  7.63E-01  2.55E+01  5.29E+03  3.71E+00  4.96E+02  7.30E-01  2.82E+01  7.05E+03  ,  A:  0.038004159927368164\n",
      "A:  0.03999757766723633\n",
      "A:  0.03696632385253906\n",
      "A:  0.044998884201049805\n",
      "A:  0.037030696868896484\n",
      "A:  0.03799557685852051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m baseModel \u001b[39m=\u001b[39m model()\n\u001b[0;32m      5\u001b[0m baseModel\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      6\u001b[0m     IndClass\u001b[39m=\u001b[39m ic,\n\u001b[0;32m      7\u001b[0m     tasks\u001b[39m=\u001b[39m t,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# dimension_strategy= DaS_strategy()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m solve \u001b[39m=\u001b[39m baseModel\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     15\u001b[0m     nb_generations \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, nb_inds_each_task\u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, \n\u001b[0;32m     16\u001b[0m     bound_pop\u001b[39m=\u001b[39;49m [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m], evaluate_initial_skillFactor\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[39m# res.append([subpop.getSolveInd().fcost for subpop in baseModel.last_pop])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 104\u001b[0m, in \u001b[0;36mmodel.fit\u001b[1;34m(self, nb_generations, nb_inds_each_task, nb_inds_max, nb_inds_min, evaluate_initial_skillFactor, c, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# print(\"E: \", end - start)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m# selection\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_inds_tasks \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(\n\u001b[0;32m    102\u001b[0m     \u001b[39mint\u001b[39m(\u001b[39mmax\u001b[39m((nb_inds_min \u001b[39m-\u001b[39m nb_inds_max) \u001b[39m*\u001b[39m (\u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_k)\u001b[39m/\u001b[39mMAXEVALS) \u001b[39m+\u001b[39m nb_inds_max, nb_inds_min))\n\u001b[0;32m    103\u001b[0m )] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtasks)\n\u001b[1;32m--> 104\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnb_inds_tasks)\n\u001b[0;32m    106\u001b[0m \u001b[39m# update operators\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossover\u001b[39m.\u001b[39mupdate(population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopulation)\n",
      "File \u001b[1;32mf:\\BTVN\\DSAI\\Optimization Lab\\pyMSOO\\pyMSOO\\utils\\Selection.py:39\u001b[0m, in \u001b[0;36mElitismSelection.__call__\u001b[1;34m(self, population, nb_inds_tasks, *args, **kwds)\u001b[0m\n\u001b[0;32m     35\u001b[0m idx_random \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(remain_idx, size\u001b[39m=\u001b[39m (N_i \u001b[39m-\u001b[39m N_elitism, ))\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     37\u001b[0m idx_selected_inds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m idx_random\n\u001b[1;32m---> 39\u001b[0m subpop\u001b[39m.\u001b[39;49mselect(idx_selected_inds)\n\u001b[0;32m     40\u001b[0m subpop\u001b[39m.\u001b[39mupdate_rank()\n\u001b[0;32m     42\u001b[0m ls_idx_selected\u001b[39m.\u001b[39mappend(idx_selected_inds)\n",
      "File \u001b[1;32mf:\\BTVN\\DSAI\\Optimization Lab\\pyMSOO\\pyMSOO\\utils\\EA.py:248\u001b[0m, in \u001b[0;36mSubPopulation.select\u001b[1;34m(self, index_selected_inds)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, index_selected_inds: \u001b[39mlist\u001b[39m):\n\u001b[0;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_inds \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_inds[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m index_selected_inds]\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactorial_rank \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactorial_rank[index_selected_inds]\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscalar_fitness \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscalar_fitness[index_selected_inds]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "# for i in range(1, 11):\n",
    "t, ic = WCCI22_benchmark.get_50tasks_benchmark(10)\n",
    "baseModel = model()\n",
    "baseModel.compile(\n",
    "    IndClass= ic,\n",
    "    tasks= t,\n",
    "    # crossover = KL_SBXCrossover(nc= 2, k= 100, conf_thres= 1),\n",
    "    crossover= SBX_Crossover(nc = 2),\n",
    "    mutation= PolynomialMutation(nm = 5),\n",
    "    selection= ElitismSelection(),\n",
    "    # dimension_strategy= DaS_strategy()\n",
    ")\n",
    "solve = baseModel.fit(\n",
    "    nb_generations = 1000, nb_inds_each_task= 100, \n",
    "    bound_pop= [0, 1], evaluate_initial_skillFactor= True\n",
    ")\n",
    "\n",
    "# res.append([subpop.getSolveInd().fcost for subpop in baseModel.last_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array([subpop.getSolveInd().fcost for subpop in baseModel.last_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.where(h < 1e-6, 0, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99465199e+01 8.43881889e+01 0.00000000e+00 8.18242387e-02\n",
      " 5.68618259e+03 1.99714880e+01 5.24616162e+01 0.00000000e+00\n",
      " 7.67451108e+00 7.61248528e+03 0.00000000e+00 6.86824019e+01\n",
      " 0.00000000e+00 1.66300205e+00 5.69074145e+03 0.00000000e+00\n",
      " 8.82043227e+01 0.00000000e+00 1.30255117e+01 7.31935204e+03\n",
      " 0.00000000e+00 8.52809900e+01 0.00000000e+00 2.59298663e-01\n",
      " 4.85861752e+03 1.99765960e+01 4.88456755e+01 0.00000000e+00\n",
      " 6.98306116e+00 7.24676037e+03 1.99238871e+01 6.52713702e+01\n",
      " 0.00000000e+00 1.09407466e+01 4.46212616e+03 1.99816756e+01\n",
      " 8.12682932e+01 0.00000000e+00 1.58228123e+00 6.75956456e+03\n",
      " 0.00000000e+00 9.92276683e+01 0.00000000e+00 1.96977725e+00\n",
      " 6.20852231e+03 1.99561195e+01 6.23440343e+01 0.00000000e+00\n",
      " 6.19344617e-02 4.94269582e+03]\n",
      "[0.00000000e+00 1.53862380e+02 0.00000000e+00 1.08909369e+01\n",
      " 5.02238391e+03 0.00000000e+00 1.12875490e+02 0.00000000e+00\n",
      " 1.23901121e+01 4.97986754e+03 2.99928333e-02 1.28012455e+02\n",
      " 0.00000000e+00 1.26848136e+01 5.05006055e+03 2.06536650e+00\n",
      " 1.36240922e+02 0.00000000e+00 1.35197252e+01 4.48117741e+03\n",
      " 0.00000000e+00 1.34383059e+02 0.00000000e+00 9.98016957e+00\n",
      " 4.40409387e+03 6.85090900e-01 1.32202483e+02 0.00000000e+00\n",
      " 1.14440587e+01 4.75557780e+03 0.00000000e+00 1.52043308e+02\n",
      " 0.00000000e+00 1.11061484e+01 4.99137256e+03 0.00000000e+00\n",
      " 1.36426152e+02 0.00000000e+00 1.04833584e+01 5.39768665e+03\n",
      " 0.00000000e+00 1.47769146e+02 2.46533333e-04 1.04713353e+01\n",
      " 5.18197074e+03 6.86528867e-01 1.13118472e+02 0.00000000e+00\n",
      " 1.04523205e+01 4.65924971e+03]\n"
     ]
    }
   ],
   "source": [
    "print(h)\n",
    "print(np.array(df[\"EME-BI\"][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h > df[\"EME-BI\"][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h < df[\"EME-BI\"][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h < f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h > f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: None -- Time: 12m 19.50s   99 % [===================>]  Pop_size: 1.00E+03  ,  Cost: 2.09E-26  1.27E-26  6.66E-23  9.66E-22  5.00E-17  2.77E-21  4.50E-26  3.78E-20  1.35E-22  1.75E-22  4.18E-25  1.19E-23  2.00E-19  9.46E-27  6.42E-24  2.59E-23  4.39E-21  1.40E-26  2.67E-19  8.13E-23  1.27E-26  1.31E-20  1.43E-16  9.88E-27  1.51E-21  1.19E-22  5.09E-24  3.03E-21  5.66E-23  1.36E-22  1.38E-24  1.33E-22  1.21E-25  7.36E-21  1.41E-20  1.59E-26  1.68E-20  1.41E-24  8.13E-27  2.26E-21  9.60E-23  1.41E-25  1.29E-21  4.46E-23  1.80E-22  6.80E-19  1.14E-19  1.73E-25  1.63E-23  1.50E-19  ,  \n",
      "END!\n",
      "Seed: None -- Time: 12m 27.17s  100 % [====================>]  Pop_size: 1.00E+03  ,  Cost: 1.99E-26  1.26E-26  5.90E-23  9.32E-22  4.99E-17  2.75E-21  1.83E-26  3.73E-20  1.27E-22  1.47E-22  1.70E-25  9.23E-24  1.99E-19  9.45E-27  4.20E-24  2.07E-23  4.34E-21  8.20E-27  2.66E-19  7.22E-23  1.48E-26  1.30E-20  1.43E-16  6.77E-27  1.47E-21  1.08E-22  2.73E-24  2.65E-21  3.54E-23  1.21E-22  6.67E-25  1.16E-22  3.54E-26  7.21E-21  1.39E-20  7.63E-27  1.67E-20  3.91E-25  6.25E-27  2.17E-21  7.98E-23  3.74E-26  1.26E-21  3.60E-23  1.69E-22  6.75E-19  1.12E-19  4.41E-26  1.30E-23  1.50E-19  ,  "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "# for i in range(1, 11):\n",
    "t, ic = WCCI22_benchmark.get_50tasks_benchmark(1)\n",
    "baseModel = model()\n",
    "baseModel.compile(\n",
    "    IndClass= ic,\n",
    "    tasks= t,\n",
    "    # crossover = KL_SBXCrossover(nc= 2, k= 100, conf_thres= 1),\n",
    "    crossover= SBX_Crossover(nc = 2),\n",
    "    mutation= PolynomialMutation(nm = 5),\n",
    "    selection= ElitismSelection(),\n",
    "    dimension_strategy= DaS_strategy()\n",
    ")\n",
    "solve = baseModel.fit(\n",
    "    nb_generations = 1000, nb_inds_each_task= 100, \n",
    "    bound_pop= [0, 1], evaluate_initial_skillFactor= True\n",
    ")\n",
    "\n",
    "# res.append([subpop.getSolveInd().fcost for subpop in baseModel.last_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([subpop.getSolveInd().fcost for subpop in baseModel.last_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00224443e-26 1.25381767e-26 5.84945741e-23 9.25842610e-22\n",
      " 4.99223604e-17 2.74625994e-21 1.67835712e-26 3.72340940e-20\n",
      " 1.25231711e-22 1.46446351e-22 1.65629568e-25 9.07643015e-24\n",
      " 1.99063843e-19 9.45460349e-27 3.91885088e-24 2.05082761e-23\n",
      " 4.33603346e-21 8.43695983e-27 2.66191771e-19 7.15704137e-23\n",
      " 1.33880265e-26 1.30275267e-20 1.43172078e-16 6.56527042e-27\n",
      " 1.46812556e-21 1.07648240e-22 2.14104324e-24 2.62973921e-21\n",
      " 3.46519705e-23 1.20577079e-22 6.29854767e-25 1.15203330e-22\n",
      " 3.30745558e-26 7.20683517e-21 1.39209230e-20 7.31792982e-27\n",
      " 1.66527030e-20 3.52280542e-25 5.90806282e-27 2.17197054e-21\n",
      " 7.94782941e-23 3.74293052e-26 1.25839868e-21 3.45773067e-23\n",
      " 1.67365666e-22 6.74356117e-19 1.12350907e-19 3.96935833e-26\n",
      " 1.27847634e-23 1.49763895e-19]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f)\n",
    "print(np.array(df[\"EME-BI\"][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.where(f < 1e-6, 0, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f < df[\"EME-BI\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f > df[\"EME-BI\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f < df[\"EME_BI_wo_DaS\"][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"EME_BI_wo_DaS\"][-50:] < df[\"EME-BI\"][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"EME_BI_wo_DaS\"][-50:] > df[\"EME-BI\"][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MFEA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
